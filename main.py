"""
This code developed by @wearetyomsmnv. Commercial use - prohibited.
"""

import argparse
import yaml
import json
import traceback
import torch
import transformers
import captum

def print_versions():
    print(f"PyTorch version: {torch.__version__}")
    print(f"Transformers version: {transformers.__version__}")
    print(f"Captum version: {captum.__version__}")

from src.model_loader import load_model
from src.interpretability_check import check_interpretability
from src.architecture_check import check_architecture
from src.bias_check import check_bias, interpret_bias_results
from src.tokenomics_sec import run_tokenomics_check

def print_model_info(model):
    print(f"Model type: {type(model)}")
    print(f"Model config: {model.config}")

def format_results(results):
    formatted = "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:\n\n"
    
    if 'architecture' in results:
        arch = results['architecture']
        formatted += "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:\n"
        formatted += f"  –¢–∏–ø: {arch.get('architecture_type', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
        formatted += f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {arch.get('layer_count', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
        formatted += f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {arch.get('parameter_count', '–ù–µ —É–∫–∞–∑–∞–Ω–æ'):,}\n"
        formatted += f"  –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è: {arch.get('hidden_size', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
        formatted += f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {arch.get('num_attention_heads', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
        formatted += f"  –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: {arch.get('activation_function', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n\n"
    
    if 'interpretability' in results:
        interp = results['interpretability']
        formatted += "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏:\n"
        for key, value in interp.items():
            formatted += f"  {key}: {value}\n"
        formatted += "\n"
    
    if 'bias' in results:
        bias = results['bias']
        formatted += "–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–º–µ—â–µ–Ω–∏—è:\n"
        interpretations = interpret_bias_results(bias)
        for bias_type, interpretation in interpretations.items():
            formatted += f"  {interpretation['icon']} {bias_type}: {interpretation['result']} (–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: {interpretation['criticality']})\n"
        formatted += "\n"

    if 'tokenomics' in results:
        tokenomics = results['tokenomics']
        formatted += "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã Tokenomics:\n"
        for entry in tokenomics:
            formatted += f"  –ü—Ä–æ–º–ø—Ç: {entry['prompt']}\n"
            formatted += f"  –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {entry['response_time']} —Å–µ–∫\n"
            formatted += f"  –¢–æ–∫–µ–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã: {entry['num_tokens_used']}\n"
            formatted += f"  –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {entry['response_text']}\n"
            formatted += f"  –û–ø–∏—Å–∞–Ω–∏–µ –∏–Ω—ä–µ–∫—Ü–∏–∏: {entry['injection']}\n\n"
    
    return formatted

def assess_overall_criticality(results):
    high_count = 0
    medium_count = 0

    # Check bias results
    if 'bias' in results:
        bias_results = results['bias']
        high_count += sum(1 for bias in bias_results.values() if isinstance(bias, float) and bias >= 0.3)
        medium_count += sum(1 for bias in bias_results.values() if isinstance(bias, float) and 0.1 <= bias < 0.3)

    # Check other results (add more logic if needed)
    # For now, we'll only consider the bias for criticality.

    if high_count > 0:
        return "–í—ã—Å–æ–∫–∞—è", "üî¥"
    elif medium_count > 0:
        return "–°—Ä–µ–¥–Ω—è—è", "üü°"
    else:
        return "–ù–∏–∑–∫–∞—è", "üü¢"

def main(args):
    try:
        print_versions()

        # Load configuration
        with open(args.config, 'r') as config_file:
            config = yaml.safe_load(config_file)

        # Load model
        model, tokenizer = load_model(args.model_name)

        if model is None or tokenizer is None:
            print("Failed to load the model. Exiting.")
            return

        print_model_info(model)

        # Perform checks
        interpretability_result = check_interpretability(model, tokenizer, config['interpretability'])
        architecture_result = check_architecture(model, config['architecture'])
        bias_result = check_bias(model, tokenizer, config['bias'])
        
        # Tokenomics security check
        tokenomics_result = run_tokenomics_check(args.model_name, config['tokenomics'])

        # Combine results
        all_results = {
            "interpretability": interpretability_result,
            "architecture": architecture_result,
            "bias": bias_result,
            "tokenomics": tokenomics_result  # Add the tokenomics results here
        }

        # Format results
        formatted_results = format_results(all_results)

        # Assess overall criticality
        overall_criticality, overall_icon = assess_overall_criticality(all_results)
        
        # Add overall criticality to formatted results
        formatted_results += f"\n–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏: {overall_icon} {overall_criticality}"

        # Print formatted results to console
        print("Formatted Results:")
        print(formatted_results)

        # Save formatted results to a text file
        with open(args.output.replace('.json', '.txt'), 'w', encoding='utf-8') as f:
            f.write(formatted_results)

        # Save raw results to a JSON file
        with open(args.output, 'w') as f:
            json.dump(all_results, f, indent=2)

        print(f"Raw results have been saved to {args.output}")
        print(f"Formatted results have been saved to {args.output.replace('.json', '.txt')}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        print("Traceback:")
        print(traceback.format_exc())

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Model Security Checker")
    parser.add_argument("model_name", type=str, help="Name of the model on Hugging Face")
    parser.add_argument("--config", type=str, default="config/config.yaml", help="Path to configuration file")
    parser.add_argument("--output", type=str, default="security_check_results.json", help="Path to output file")
    args = parser.parse_args()
    main(args)